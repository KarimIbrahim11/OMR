**********************
Windows PowerShell transcript start
Start time: 20170608210832
Username: MONSTI\Alex
RunAs User: MONSTI\Alex
Machine: MONSTI (Microsoft Windows NT 10.0.15063.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\MusicSymbolClassifier\TrainModel.ps1'
Process ID: 21236
PSVersion: 5.1.15063.296
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.15063.296
BuildVersion: 10.0.15063.296
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\2017-06-08_vgg_staff88_adadelta_no_lr_reduction.txt
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses ([3]) and with staff-lines with 1 different offsets from the top ([88])
In directory C:\Users\Alex\Repositories\MusicSymbolClassifier\HomusTrainer\data\images
15200/15200Deleting split directories...
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 16)      448
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 16)      64
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 16)      0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 16)      2320
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 16)      64
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 16)      0
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 16)       0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 32)       4640
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 32)       128
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 32)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 32)       9248
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 32)       128
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 32)       0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 32)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 64)        18496
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 64)        256
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 64)        0
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 64)        36928
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 64)        256
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 64)        0
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 64)        36928
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 64)        256
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 64)        0
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 64)        0
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 128)       73856
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 128)       512
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 128)       0
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 128)       147584
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 128)       512
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 128)       0
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 128)       147584
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 128)       512
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 128)       0
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 128)        0
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 192)        221376
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 192)        768
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 192)        0
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 192)        331968
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 192)        768
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 192)        0
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 192)        331968
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 192)        768
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 192)        0
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 8, 192)        331968
_________________________________________________________________
batch_normalization_14 (Batc (None, 14, 8, 192)        768
_________________________________________________________________
activation_14 (Activation)   (None, 14, 8, 192)        0
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 4, 192)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064
_________________________________________________________________
output_node (Activation)     (None, 32)                0
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-06-08 21:12:47.936400: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.936518: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.936908: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.938658: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.941318: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.941756: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.942701: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:47.943060: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] Th
e TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-08 21:12:48.161919: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887
] Found device 0 with properties:
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2017-06-08 21:12:48.162024: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908
] DMA: 0
2017-06-08 21:12:48.163918: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918
] 0:   Y
2017-06-08 21:12:48.164406: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977
] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 32, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'epsilon': 1e-08, 'decay': 0.0, 'lr': 1.0, 'rho': 0.95}
Learning-rate reduction on Plateau disabled
Epoch 1/200
  9/381 [..............................] - ETA: 353s - loss: 6.3244 - acc: 0.03472017-06-08 21:13:04.354495: I c:\tf_jenkins\home\workspace\relea
se-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3612 get requests, put_count=3524 evi
cted_count=1000 eviction_rate=0.283768 and unsatisfied allocation rate=0.328904
2017-06-08 21:13:04.354585: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc
:259] Raising pool_size_limit_ from 100 to 110
380/381 [============================>.] - ETA: 0s - loss: 1.9402 - acc: 0.4799Epoch 00000: val_acc improved from -inf to 0.26007, saving model t
o 2017-06-08_vgg.h5
381/381 [==============================] - 157s - loss: 1.9364 - acc: 0.4813 - val_loss: 4.7301 - val_acc: 0.2601
Epoch 2/200
380/381 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7512Epoch 00001: val_acc improved from 0.26007 to 0.42310, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 150s - loss: 0.8647 - acc: 0.7515 - val_loss: 4.8446 - val_acc: 0.4231
Epoch 3/200
380/381 [============================>.] - ETA: 0s - loss: 0.6573 - acc: 0.8172Epoch 00002: val_acc did not improve
381/381 [==============================] - 150s - loss: 0.6567 - acc: 0.8174 - val_loss: 5.6994 - val_acc: 0.3314
Epoch 4/200
380/381 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.8477Epoch 00003: val_acc improved from 0.42310 to 0.77954, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.5658 - acc: 0.8478 - val_loss: 0.7194 - val_acc: 0.7795
Epoch 5/200
380/381 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8670Epoch 00004: val_acc improved from 0.77954 to 0.83234, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.5077 - acc: 0.8671 - val_loss: 0.6435 - val_acc: 0.8323
Epoch 6/200
380/381 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8879Epoch 00005: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.4584 - acc: 0.8872 - val_loss: 1.2519 - val_acc: 0.7340
Epoch 7/200
380/381 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8998Epoch 00006: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.4261 - acc: 0.8998 - val_loss: 0.9081 - val_acc: 0.7729
Epoch 8/200
380/381 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.9044Epoch 00007: val_acc improved from 0.83234 to 0.88647, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 152s - loss: 0.4025 - acc: 0.9047 - val_loss: 0.4671 - val_acc: 0.8865
Epoch 9/200
380/381 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.9190Epoch 00008: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.3590 - acc: 0.9189 - val_loss: 0.4971 - val_acc: 0.8713
Epoch 10/200
380/381 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.9201Epoch 00009: val_acc improved from 0.88647 to 0.90429, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.3535 - acc: 0.9204 - val_loss: 0.4386 - val_acc: 0.9043
Epoch 11/200
380/381 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.9310Epoch 00010: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.3305 - acc: 0.9309 - val_loss: 0.7773 - val_acc: 0.8145
Epoch 12/200
380/381 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9339Epoch 00011: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.3172 - acc: 0.9338 - val_loss: 0.8022 - val_acc: 0.8178
Epoch 13/200
380/381 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9414Epoch 00012: val_acc improved from 0.90429 to 0.92013, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.3051 - acc: 0.9416 - val_loss: 0.3940 - val_acc: 0.9201
Epoch 14/200
380/381 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9430Epoch 00013: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.2924 - acc: 0.9429 - val_loss: 0.6475 - val_acc: 0.8502
Epoch 15/200
380/381 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9494Epoch 00014: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.2793 - acc: 0.9493 - val_loss: 0.4281 - val_acc: 0.9003
Epoch 16/200
380/381 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9521Epoch 00015: val_acc improved from 0.92013 to 0.92475, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 152s - loss: 0.2698 - acc: 0.9523 - val_loss: 0.3903 - val_acc: 0.9248
Epoch 17/200
380/381 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9559Epoch 00016: val_acc improved from 0.92475 to 0.92805, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.2624 - acc: 0.9560 - val_loss: 0.3568 - val_acc: 0.9281
Epoch 18/200
380/381 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9555Epoch 00017: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.2586 - acc: 0.9556 - val_loss: 0.3934 - val_acc: 0.9182
Epoch 19/200
380/381 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9609Epoch 00018: val_acc improved from 0.92805 to 0.94257, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 151s - loss: 0.2428 - acc: 0.9610 - val_loss: 0.3156 - val_acc: 0.9426
Epoch 20/200
380/381 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9636Epoch 00019: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.2363 - acc: 0.9637 - val_loss: 0.3685 - val_acc: 0.9254
Epoch 21/200
380/381 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9663Epoch 00020: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2270 - acc: 0.9664 - val_loss: 0.4027 - val_acc: 0.9155
Epoch 22/200
380/381 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9657Epoch 00021: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2238 - acc: 0.9658 - val_loss: 0.4784 - val_acc: 0.9023
Epoch 23/200
380/381 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9677Epoch 00022: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2196 - acc: 0.9675 - val_loss: 0.4383 - val_acc: 0.9215
Epoch 24/200
380/381 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9714Epoch 00023: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2104 - acc: 0.9712 - val_loss: 0.7234 - val_acc: 0.8620
Epoch 25/200
380/381 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9737Epoch 00024: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2038 - acc: 0.9735 - val_loss: 0.4473 - val_acc: 0.9208
Epoch 26/200
380/381 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9735Epoch 00025: val_acc improved from 0.94257 to 0.94653, saving mode
l to 2017-06-08_vgg.h5
381/381 [==============================] - 152s - loss: 0.1989 - acc: 0.9733 - val_loss: 0.2997 - val_acc: 0.9465
Epoch 27/200
380/381 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9743Epoch 00026: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.2000 - acc: 0.9744 - val_loss: 0.3900 - val_acc: 0.9129
Epoch 28/200
380/381 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9754Epoch 00027: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1960 - acc: 0.9755 - val_loss: 0.3322 - val_acc: 0.9373
Epoch 29/200
380/381 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9748Epoch 00028: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1958 - acc: 0.9748 - val_loss: 0.3356 - val_acc: 0.9380
Epoch 30/200
380/381 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9748Epoch 00029: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1941 - acc: 0.9748 - val_loss: 0.3310 - val_acc: 0.9294
Epoch 31/200
380/381 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9779Epoch 00030: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1850 - acc: 0.9779 - val_loss: 0.4089 - val_acc: 0.9234
Epoch 32/200
380/381 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9767Epoch 00031: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1858 - acc: 0.9768 - val_loss: 0.3428 - val_acc: 0.9333
Epoch 33/200
380/381 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9795Epoch 00032: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1764 - acc: 0.9796 - val_loss: 0.3955 - val_acc: 0.9327
Epoch 34/200
380/381 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9771Epoch 00033: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1828 - acc: 0.9772 - val_loss: 0.3424 - val_acc: 0.9294
Epoch 35/200
380/381 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9813Epoch 00034: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1737 - acc: 0.9814 - val_loss: 0.3728 - val_acc: 0.9241
Epoch 36/200
380/381 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9806Epoch 00035: val_acc did not improve
381/381 [==============================] - 152s - loss: 0.1710 - acc: 0.9806 - val_loss: 0.3178 - val_acc: 0.9465
Epoch 37/200
380/381 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9802Epoch 00036: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1703 - acc: 0.9802 - val_loss: 0.3184 - val_acc: 0.9393
Epoch 38/200
380/381 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9830Epoch 00037: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1681 - acc: 0.9830 - val_loss: 0.3545 - val_acc: 0.9386
Epoch 39/200
380/381 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9829Epoch 00038: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1672 - acc: 0.9829 - val_loss: 0.3853 - val_acc: 0.9261
Epoch 40/200
380/381 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9849Epoch 00039: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1634 - acc: 0.9849 - val_loss: 0.3359 - val_acc: 0.9327
Epoch 41/200
380/381 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9856Epoch 00040: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1559 - acc: 0.9856 - val_loss: 0.3403 - val_acc: 0.9327
Epoch 42/200
380/381 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9850Epoch 00041: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1540 - acc: 0.9851 - val_loss: 0.3874 - val_acc: 0.9109
Epoch 43/200
380/381 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9853Epoch 00042: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1566 - acc: 0.9853 - val_loss: 0.3667 - val_acc: 0.9347
Epoch 44/200
380/381 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9857Epoch 00043: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1521 - acc: 0.9855 - val_loss: 0.4780 - val_acc: 0.9050
Epoch 45/200
380/381 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9837Epoch 00044: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1564 - acc: 0.9835 - val_loss: 0.3285 - val_acc: 0.9426
Epoch 46/200
380/381 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9882Epoch 00045: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1414 - acc: 0.9883 - val_loss: 0.3550 - val_acc: 0.9347
Epoch 47/200
380/381 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9865Epoch 00046: val_acc did not improve
381/381 [==============================] - 151s - loss: 0.1464 - acc: 0.9865 - val_loss: 0.3673 - val_acc: 0.9281
Epoch 00046: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

      12-8-Time       0.98      1.00      0.99        40
       2-2-Time       1.00      0.97      0.99        39
       2-4-Time       0.89      0.97      0.93        40
       3-4-Time       1.00      0.82      0.90        40
       3-8-Time       0.95      1.00      0.98        40
       4-4-Time       0.97      0.97      0.97        40
       6-8-Time       1.00      0.97      0.99        40
       9-8-Time       0.97      0.97      0.97        40
        Barline       1.00      0.97      0.99        40
         C-Clef       0.93      1.00      0.96        40
    Common-Time       1.00      0.95      0.97        40
       Cut-Time       1.00      0.97      0.99        40
            Dot       0.89      1.00      0.94        40
   Double-Sharp       0.95      0.95      0.95        40
    Eighth-Note       0.93      0.95      0.94        80
    Eighth-Rest       0.97      0.88      0.92        40
         F-Clef       0.98      1.00      0.99        40
           Flat       0.95      1.00      0.97        39
         G-Clef       0.98      1.00      0.99        40
      Half-Note       1.00      0.96      0.98        79
        Natural       0.95      0.97      0.96        40
   Quarter-Note       0.93      1.00      0.96        80
   Quarter-Rest       0.94      0.82      0.88        40
          Sharp       0.88      0.95      0.92        40
 Sixteenth-Note       0.80      0.89      0.84        80
 Sixteenth-Rest       0.84      0.90      0.87        40
Sixty-Four-Note       0.89      0.89      0.89        79
Sixty-Four-Rest       0.84      0.90      0.87        40
Thirty-Two-Note       0.83      0.68      0.75        79
Thirty-Two-Rest       0.85      0.72      0.78        40
Whole-Half-Rest       0.97      0.88      0.92        40
     Whole-Note       0.93      1.00      0.96        40

    avg / total       0.93      0.93      0.93      1515

Total Loss: 0.34054
Total Accuracy: 92.93729%
Total Error: 7.06271%
Execution time: 7177.3s
**********************
Windows PowerShell transcript end
End time: 20170608231224
**********************
