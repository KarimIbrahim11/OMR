C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py --delete_and_recreate_dataset_directory False --model_name vgg4 --height 192 --width 96
Using TensorFlow backend.
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 32)      9248      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 64)       18496     
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 64)       36928     
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 128)       73856     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 256)       295168    
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 256)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 256)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 512)        1180160   
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
average_pooling2d_1 (Average (None, 7, 4, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 14336)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                458784    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 8,280,000
Trainable params: 8,274,240
Non-trainable params: 5,760
_________________________________________________________________
Model vgg4 loaded.
2017-06-04 19:08:21.941330: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.941590: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.941832: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.942127: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.942403: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.942737: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.942981: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:21.943229: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-04 19:08:22.254251: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-04 19:08:22.254584: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-06-04 19:08:22.254750: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-06-04 19:08:22.254920: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 64, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10° randomly
Optimizer: Adadelta, with parameters {'lr': 1.0, 'rho': 0.95, 'epsilon': 1e-08, 'decay': 0.0}
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

      12-8-Time       1.00      1.00      1.00        40
       2-2-Time       1.00      1.00      1.00        39
       2-4-Time       0.97      0.97      0.97        40
       3-4-Time       1.00      0.97      0.99        40
       3-8-Time       1.00      0.97      0.99        40
       4-4-Time       0.97      0.97      0.97        40
       6-8-Time       1.00      1.00      1.00        40
       9-8-Time       0.97      0.97      0.97        40
        Barline       0.98      1.00      0.99        40
         C-Clef       1.00      1.00      1.00        40
    Common-Time       1.00      1.00      1.00        40
       Cut-Time       1.00      0.97      0.99        40
            Dot       1.00      0.97      0.99        40
   Double-Sharp       1.00      0.97      0.99        40
    Eighth-Note       0.97      0.96      0.97        80
    Eighth-Rest       0.98      1.00      0.99        40
         F-Clef       1.00      1.00      1.00        40
           Flat       1.00      1.00      1.00        39
         G-Clef       0.98      1.00      0.99        40
      Half-Note       1.00      0.97      0.99        79
        Natural       1.00      0.97      0.99        40
   Quarter-Note       0.98      1.00      0.99        80
   Quarter-Rest       0.95      0.95      0.95        40
          Sharp       0.97      0.95      0.96        40
 Sixteenth-Note       0.92      0.97      0.95        80
 Sixteenth-Rest       0.95      0.95      0.95        40
Sixty-Four-Note       0.96      0.94      0.95        79
Sixty-Four-Rest       0.95      0.95      0.95        40
Thirty-Two-Note       0.94      0.92      0.93        79
Thirty-Two-Rest       0.95      0.97      0.96        40
Whole-Half-Rest       0.95      1.00      0.98        40
     Whole-Note       1.00      1.00      1.00        40

    avg / total       0.98      0.98      0.98      1515

Total Loss: 0.17686
Total Accuracy: 97.62376%
Total Error: 2.37624%
Execution time: 13.3s

Process finished with exit code 1
