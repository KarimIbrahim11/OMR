C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/ModelGenerator/TrainModel.py --delete_and_recreate_dataset_directory False --model_name vgg4
Using TensorFlow backend.
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 244, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 244, 128, 32)      128       
_________________________________________________________________
activation_1 (Activation)    (None, 244, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 244, 128, 32)      9248      
_________________________________________________________________
batch_normalization_2 (Batch (None, 244, 128, 32)      128       
_________________________________________________________________
activation_2 (Activation)    (None, 244, 128, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 122, 64, 32)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 122, 64, 64)       18496     
_________________________________________________________________
batch_normalization_3 (Batch (None, 122, 64, 64)       256       
_________________________________________________________________
activation_3 (Activation)    (None, 122, 64, 64)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 122, 64, 64)       36928     
_________________________________________________________________
batch_normalization_4 (Batch (None, 122, 64, 64)       256       
_________________________________________________________________
activation_4 (Activation)    (None, 122, 64, 64)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 61, 32, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 61, 32, 128)       73856     
_________________________________________________________________
batch_normalization_5 (Batch (None, 61, 32, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 61, 32, 128)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 61, 32, 128)       147584    
_________________________________________________________________
batch_normalization_6 (Batch (None, 61, 32, 128)       512       
_________________________________________________________________
activation_6 (Activation)    (None, 61, 32, 128)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 61, 32, 128)       147584    
_________________________________________________________________
batch_normalization_7 (Batch (None, 61, 32, 128)       512       
_________________________________________________________________
activation_7 (Activation)    (None, 61, 32, 128)       0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 30, 16, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 30, 16, 256)       295168    
_________________________________________________________________
batch_normalization_8 (Batch (None, 30, 16, 256)       1024      
_________________________________________________________________
activation_8 (Activation)    (None, 30, 16, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 30, 16, 256)       590080    
_________________________________________________________________
batch_normalization_9 (Batch (None, 30, 16, 256)       1024      
_________________________________________________________________
activation_9 (Activation)    (None, 30, 16, 256)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 30, 16, 256)       590080    
_________________________________________________________________
batch_normalization_10 (Batc (None, 30, 16, 256)       1024      
_________________________________________________________________
activation_10 (Activation)   (None, 30, 16, 256)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 15, 8, 256)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 15, 8, 512)        1180160   
_________________________________________________________________
batch_normalization_11 (Batc (None, 15, 8, 512)        2048      
_________________________________________________________________
activation_11 (Activation)   (None, 15, 8, 512)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 15, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 15, 8, 512)        2048      
_________________________________________________________________
activation_12 (Activation)   (None, 15, 8, 512)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 15, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_13 (Batc (None, 15, 8, 512)        2048      
_________________________________________________________________
activation_13 (Activation)   (None, 15, 8, 512)        0         
_________________________________________________________________
average_pooling2d_1 (Average (None, 7, 4, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 14336)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                458784    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 8,280,000
Trainable params: 8,274,240
Non-trainable params: 5,760
_________________________________________________________________
Model vgg4 loaded.
Training for 200 epochs with initial learning rate of 0.001, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: he_normal, Minibatch-size: 32, Early stopping after 10 epochs without improvement
Data-Shape: (244, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 5 epochs
Epoch 1/200
2017-05-26 18:58:27.041198: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.041468: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.041746: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.042026: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.042269: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.042503: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.042746: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.043001: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-26 18:58:27.375515: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-05-26 18:58:27.375883: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-05-26 18:58:27.375986: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-05-26 18:58:27.376121: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
 10/381 [..............................] - ETA: 276s - loss: 3.6075 - acc: 0.10002017-05-26 18:58:34.379941: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3727 get requests, put_count=3703 evicted_count=1000 eviction_rate=0.270051 and unsatisfied allocation rate=0.301583
2017-05-26 18:58:34.380246: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
380/381 [============================>.] - ETA: 0s - loss: 1.1812 - acc: 0.7129Epoch 00000: val_acc improved from -inf to 0.70297, saving model to vgg4.h5
381/381 [==============================] - 84s - loss: 1.1800 - acc: 0.7134 - val_loss: 1.3146 - val_acc: 0.7030
Epoch 2/200
380/381 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8715Epoch 00001: val_acc improved from 0.70297 to 0.86007, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.6307 - acc: 0.8716 - val_loss: 0.6243 - val_acc: 0.8601
Epoch 3/200
380/381 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.9016Epoch 00002: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.5306 - acc: 0.9016 - val_loss: 0.7086 - val_acc: 0.8422
Epoch 4/200
380/381 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.9212Epoch 00003: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.4789 - acc: 0.9214 - val_loss: 0.8727 - val_acc: 0.8205
Epoch 5/200
380/381 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.9376Epoch 00004: val_acc improved from 0.86007 to 0.90693, saving model to vgg4.h5
381/381 [==============================] - 74s - loss: 0.4369 - acc: 0.9377 - val_loss: 0.5610 - val_acc: 0.9069
Epoch 6/200
380/381 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.9432Epoch 00005: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.4146 - acc: 0.9433 - val_loss: 0.7095 - val_acc: 0.8429
Epoch 7/200
380/381 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.9533Epoch 00006: val_acc improved from 0.90693 to 0.92343, saving model to vgg4.h5
381/381 [==============================] - 74s - loss: 0.3932 - acc: 0.9534 - val_loss: 0.4927 - val_acc: 0.9234
Epoch 8/200
380/381 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.9567Epoch 00007: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3792 - acc: 0.9565 - val_loss: 0.4921 - val_acc: 0.9168
Epoch 9/200
380/381 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.9591Epoch 00008: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3755 - acc: 0.9590 - val_loss: 1.0819 - val_acc: 0.7340
Epoch 10/200
380/381 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.9642Epoch 00009: val_acc improved from 0.92343 to 0.93531, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.3610 - acc: 0.9641 - val_loss: 0.4423 - val_acc: 0.9353
Epoch 11/200
380/381 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.9669Epoch 00010: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3521 - acc: 0.9669 - val_loss: 0.6714 - val_acc: 0.8799
Epoch 12/200
380/381 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.9732Epoch 00011: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3363 - acc: 0.9730 - val_loss: 1.0712 - val_acc: 0.7888
Epoch 13/200
380/381 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.9703Epoch 00012: val_acc improved from 0.93531 to 0.95578, saving model to vgg4.h5
381/381 [==============================] - 74s - loss: 0.3411 - acc: 0.9704 - val_loss: 0.3845 - val_acc: 0.9558
Epoch 14/200
380/381 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.9767Epoch 00013: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3232 - acc: 0.9768 - val_loss: 0.4032 - val_acc: 0.9551
Epoch 15/200
380/381 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.9768Epoch 00014: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3231 - acc: 0.9769 - val_loss: 0.5269 - val_acc: 0.9149
Epoch 16/200
380/381 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9805Epoch 00015: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3147 - acc: 0.9806 - val_loss: 0.4385 - val_acc: 0.9380
Epoch 17/200
380/381 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9808Epoch 00016: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3134 - acc: 0.9806 - val_loss: 0.4299 - val_acc: 0.9492
Epoch 18/200
380/381 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9845Epoch 00017: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.3023 - acc: 0.9845 - val_loss: 0.3731 - val_acc: 0.9558
Epoch 19/200
380/381 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9855Epoch 00018: val_acc did not improve

Epoch 00018: reducing learning rate to 0.0005000000237487257.
381/381 [==============================] - 75s - loss: 0.3022 - acc: 0.9856 - val_loss: 0.4519 - val_acc: 0.9452
Epoch 20/200
380/381 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9918Epoch 00019: val_acc improved from 0.95578 to 0.96304, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.2835 - acc: 0.9918 - val_loss: 0.3783 - val_acc: 0.9630
Epoch 21/200
380/381 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9939Epoch 00020: val_acc improved from 0.96304 to 0.96700, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.2789 - acc: 0.9939 - val_loss: 0.3466 - val_acc: 0.9670
Epoch 22/200
380/381 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9940Epoch 00021: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2784 - acc: 0.9940 - val_loss: 0.3791 - val_acc: 0.9637
Epoch 23/200
380/381 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9938Epoch 00022: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2788 - acc: 0.9938 - val_loss: 0.3922 - val_acc: 0.9578
Epoch 24/200
380/381 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9952Epoch 00023: val_acc improved from 0.96700 to 0.97228, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.2754 - acc: 0.9950 - val_loss: 0.3555 - val_acc: 0.9723
Epoch 25/200
380/381 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9926Epoch 00024: val_acc improved from 0.97228 to 0.97624, saving model to vgg4.h5
381/381 [==============================] - 75s - loss: 0.2781 - acc: 0.9926 - val_loss: 0.3528 - val_acc: 0.9762
Epoch 26/200
380/381 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9963Epoch 00025: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2726 - acc: 0.9963 - val_loss: 0.4057 - val_acc: 0.9551
Epoch 27/200
380/381 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9950Epoch 00026: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2743 - acc: 0.9950 - val_loss: 0.3624 - val_acc: 0.9756
Epoch 28/200
380/381 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9961Epoch 00027: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2718 - acc: 0.9961 - val_loss: 0.3629 - val_acc: 0.9683
Epoch 29/200
380/381 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9967Epoch 00028: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2699 - acc: 0.9967 - val_loss: 0.3382 - val_acc: 0.9756
Epoch 30/200
380/381 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9958Epoch 00029: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2711 - acc: 0.9958 - val_loss: 0.3612 - val_acc: 0.9644
Epoch 31/200
380/381 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9969Epoch 00030: val_acc did not improve

Epoch 00030: reducing learning rate to 0.0002500000118743628.
381/381 [==============================] - 74s - loss: 0.2672 - acc: 0.9969 - val_loss: 0.3594 - val_acc: 0.9723
Epoch 32/200
380/381 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9962Epoch 00031: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2672 - acc: 0.9962 - val_loss: 0.3515 - val_acc: 0.9716
Epoch 33/200
380/381 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9984Epoch 00032: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2649 - acc: 0.9984 - val_loss: 0.3502 - val_acc: 0.9749
Epoch 34/200
380/381 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9984Epoch 00033: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2632 - acc: 0.9984 - val_loss: 0.3826 - val_acc: 0.9670
Epoch 35/200
380/381 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9975Epoch 00034: val_acc did not improve
381/381 [==============================] - 74s - loss: 0.2653 - acc: 0.9972 - val_loss: 0.3371 - val_acc: 0.9756
Epoch 36/200
380/381 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9970Epoch 00035: val_acc did not improve

Epoch 00035: reducing learning rate to 0.0001250000059371814.
381/381 [==============================] - 74s - loss: 0.2656 - acc: 0.9970 - val_loss: 0.3451 - val_acc: 0.9743
Epoch 00035: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

      12-8-Time       1.00      1.00      1.00        40
       2-2-Time       1.00      1.00      1.00        39
       2-4-Time       0.97      0.97      0.97        40
       3-4-Time       1.00      0.97      0.99        40
       3-8-Time       1.00      1.00      1.00        40
       4-4-Time       0.98      1.00      0.99        40
       6-8-Time       1.00      1.00      1.00        40
       9-8-Time       1.00      0.97      0.99        40
        Barline       0.98      1.00      0.99        40
         C-Clef       1.00      1.00      1.00        40
    Common-Time       1.00      1.00      1.00        40
       Cut-Time       1.00      1.00      1.00        40
            Dot       0.95      0.97      0.96        40
   Double-Sharp       1.00      1.00      1.00        40
    Eighth-Note       0.99      0.96      0.97        80
    Eighth-Rest       1.00      0.97      0.99        40
         F-Clef       0.98      1.00      0.99        40
           Flat       1.00      1.00      1.00        39
         G-Clef       1.00      1.00      1.00        40
      Half-Note       1.00      1.00      1.00        79
        Natural       0.97      0.95      0.96        40
   Quarter-Note       1.00      1.00      1.00        80
   Quarter-Rest       0.90      0.95      0.93        40
          Sharp       1.00      0.95      0.97        40
 Sixteenth-Note       0.91      0.97      0.94        80
 Sixteenth-Rest       0.92      0.90      0.91        40
Sixty-Four-Note       0.96      0.94      0.95        79
Sixty-Four-Rest       1.00      0.95      0.97        40
Thirty-Two-Note       0.92      0.92      0.92        79
Thirty-Two-Rest       0.93      0.95      0.94        40
Whole-Half-Rest       0.95      0.95      0.95        40
     Whole-Note       1.00      1.00      1.00        40

    avg / total       0.98      0.98      0.98      1515

Total Loss: 0.34946
Total Accuracy: 97.55776%
Total Error: 2.44224%
Execution time: 2723.9s

Process finished with exit code 0
